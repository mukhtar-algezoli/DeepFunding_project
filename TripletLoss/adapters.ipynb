{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# work_dir = '/content/drive/MyDrive/DeepFund/'\n",
    "# os.chdir(work_dir)\n",
    "# os.chdir('DeepFunding_project/TripletLoss')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from triplet_dataset import get_dataset, get_sentence_id_label_df, TripletDataset\n",
    "from network import get_sts_model\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from model_evaluation import  compare_sactter_plots\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "def calculate_dsiatances_from_embeddings(embeddings, labels):\n",
    "    # Calculate average distances within and across groups\n",
    "    group_distances = []\n",
    "    total_distances = []\n",
    "    all_res = {\n",
    "        'group_id': [],\n",
    "        'group_distance': [],\n",
    "        'total_distance': [],\n",
    "    }\n",
    "    unique_groups = labels.unique()\n",
    "\n",
    "    for group in unique_groups:\n",
    "        group_indices = labels[labels == group].index\n",
    "        group_embeddings = embeddings[group_indices]\n",
    "        \n",
    "        # Calculate pairwise cosine distances within the group\n",
    "        group_distance = cosine_distances(group_embeddings).mean()\n",
    "        group_distances.append(group_distance)\n",
    "        \n",
    "        # Calculate pairwise cosine distances across groups\n",
    "        other_indices = labels[labels != group].index\n",
    "        other_embeddings = embeddings[other_indices]\n",
    "        total_distance = cosine_distances(group_embeddings, other_embeddings).mean()\n",
    "        total_distances.append(total_distance)\n",
    "\n",
    "        all_res['group_id'].append(group)\n",
    "        all_res['group_distance'].append(group_distance)\n",
    "        all_res['total_distance'].append(total_distance)\n",
    "\n",
    "    # Calculate the average distances\n",
    "    average_group_distance = sum(group_distances) / len(group_distances)\n",
    "    average_total_distance = sum(total_distances) / len(total_distances)\n",
    "\n",
    "    print(\"Average distance within groups:\", average_group_distance)\n",
    "    print(\"Average distance across groups:\", average_total_distance)\n",
    "\n",
    "    return average_group_distance, average_total_distance, all_res\n",
    "\n",
    "\n",
    "def gen_vis_embeddings(no_peft=False):\n",
    "    embeddings = []\n",
    "    sentences = VIS_DATA['sentence'].tolist()\n",
    "    for sentence in tqdm(sentences, unit='sentence', desc='Generating embeddings'):\n",
    "        if no_peft:\n",
    "            print('Do Something')\n",
    "        else:\n",
    "            embedding = MODEL(sentence).detach().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    embeddings = np.array(embeddings).squeeze()\n",
    "    embeddings = TSNE(n_components=2).fit_transform(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def main(batch_size=16):\n",
    "    # Model without\n",
    "    # bare_embeddings = gen_vis_embeddings(no_peft=True)\n",
    "    # ids = VIS_DATA['id'].tolist()\n",
    "    # print('Bare model Clustering: ')\n",
    "    # compare_sactter_plots(bare_embeddings, None, ids)\n",
    "    # print('='*100)\n",
    "    optimizer = torch.optim.AdamW(params=MODEL.parameters(), lr=1e-5)\n",
    "    triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    num_epochs = 5\n",
    "    eval_every = 100\n",
    "    save_model_every = 1000\n",
    "    print('Training model...')\n",
    "    train(batch_size, optimizer, triplet_loss, num_epochs, eval_every, save_model_every)\n",
    "\n",
    "\n",
    "def train(batch_size,optimizer, triplet_loss, num_epochs, eval_every, save_model_every):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_dataset = TripletDataset(DATA, tokenizer=TOKENIZER, device=DEVICE, batch_size=batch_size, shuffle=True, max_len=100)\n",
    "        loss = 0\n",
    "        steps = 0\n",
    "        accumelated_loss = 0\n",
    "        tbar = tqdm(train_dataset, unit='batch')\n",
    "        for input in tbar:\n",
    "                steps += 1\n",
    "                anchor = MODEL(input[0])\n",
    "                positive = MODEL(input[1])\n",
    "                negative = MODEL(input[2])\n",
    "                loss = triplet_loss(anchor, positive, negative)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                accumelated_loss += loss.item()\n",
    "                tbar.set_description(f'Epoch {epoch} current loss: {loss:.2f} avaerage loss: {accumelated_loss/steps:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "                if steps % eval_every == 0:\n",
    "                    print('Evaluating model')\n",
    "                    print(f'Avaerage loss: {accumelated_loss/steps:.2f} ')\n",
    "                    embeddings = gen_vis_embeddings()\n",
    "                    labels = VIS_DATA['label']\n",
    "                    d = calculate_dsiatances_from_embeddings(embeddings, labels)\n",
    "\n",
    "                    \n",
    "                if steps % 500 == 0:\n",
    "                    accumelated_loss = 0\n",
    "                    embeddings = gen_vis_embeddings()\n",
    "                    ids = VIS_DATA['id'].tolist()\n",
    "                    print('Model Clustering: ')\n",
    "                    compare_sactter_plots(embeddings,None, ids)\n",
    "                    print('='*100)\n",
    "\n",
    "                if steps % save_model_every == 0:\n",
    "                    print('Saving model')\n",
    "                    if not os.path.exists('./models'):\n",
    "                        os.makedirs('./models')\n",
    "\n",
    "                    lora_model = MODEL.Bert_representations\n",
    "\n",
    "                    if not os.path.exists('./models/LoRa'):\n",
    "                        os.makedirs('./models/LoRa')\n",
    "\n",
    "                    #model name without / character\n",
    "                    short_model_name = MODEL_PATH.split('/')[-1]\n",
    "                    lora_save_path = f'./models/LoRa/lora_model_{short_model_name}_{steps}'\n",
    "                    lora_model.save_pretrained(lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5357568 || all params: 38717568 || trainable%: 13.837563351086514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4596da66262a4c83b187b7e241622c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating triplets:   0%|          | 0/14 [00:00<?, ?group/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "RANK = 64\n",
    "PEFT_CONFIG = LoraConfig(inference_mode=False,\n",
    "              r=RANK,\n",
    "              lora_alpha=RANK*2,\n",
    "              lora_dropout=0.05,\n",
    "              target_modules=['value','query','key', 'dense']\n",
    "              )\n",
    "global DEVICE\n",
    "global MODEL_PATH\n",
    "global MODEL\n",
    "global TOKENIZER\n",
    "global DATA\n",
    "global VIS_DATA\n",
    "DEVICE = 'cuda'\n",
    "MODEL_PATH = 'sentence-transformers/all-MiniLM-L12-v1'\n",
    "MODEL = get_sts_model(model_path=MODEL_PATH, device=DEVICE, pef_config=PEFT_CONFIG)\n",
    "TOKENIZER = MODEL.tokenizer\n",
    "DATA = get_dataset()\n",
    "VIS_DATA = get_sentence_id_label_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a571e21b764244894d0e54de226a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/257 [00:00<?, ?sentence/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance within groups: 0.7796884860311236\n",
      "Average distance across groups: 1.012198371546609\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gen_vis_embeddings()\n",
    "ids = VIS_DATA['id'].tolist()\n",
    "print('Model Clustering: ')\n",
    "compare_sactter_plots(embeddings,None, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print layers names in model\n",
    "for name, param in MODEL.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
